apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: helical-raycluster
  namespace: helical-pdqueiros
spec:
  # minimum specs to limit it to cluster management
  headGroupSpec:
    rayStartParams:
      num-cpus: "0"
    template:
      spec:
        containers:
        - name: ray-head
          image: rayproject/ray:2.5.0
          resources:
            limits:
              cpu: "1"
              # we could play around with this, but let's set it to 2gb for stability sake
              memory: "2Gi"
            requests:
              cpu: "1"
              memory: "2Gi"

  # note that I've set the worker specs to my local machine but this ought to change in a cloud env
  workerGroupSpecs:
  # ------------------------------------------------------------------
  # 1. GPU Worker Group (for ML/Deep Learning tasks)
  # ------------------------------------------------------------------
  - groupName: gpu-worker-group
    replicas: 0           # Start with 1 GPU worker
    minReplicas: 0
    maxReplicas: 1        # Auto-scale up to 5 GPU workers
    template:
      spec:
        # Use a Ray image that includes CUDA and ML libraries
        containers:
        - name: ray-worker
          image: rayproject/ray-ml:2.5.0-gpu  # <--- CRITICAL: Use a GPU image
          resources:
            requests:
              cpu: "16"
              memory: "16Gi"
              nvidia.com/gpu: "1"  # <--- CRITICAL: Request 1 GPU
            limits:
              cpu: "20"
              memory: "16Gi"
              nvidia.com/gpu: "1"  # <--- CRITICAL: Request 1 GPU

        # OPTIONAL: Use nodeSelector to force these pods onto GPU-enabled nodes
        nodeSelector:
          # This label may vary based on your cloud/cluster setup
          gpu: "true"

  # ------------------------------------------------------------------
  # 2. CPU Worker Group (for general purpose tasks)
  # ------------------------------------------------------------------
  - groupName: cpu-worker-group
    # im using 1 replica as a starting point for testing purposes
    replicas: 1
    minReplicas: 1
    maxReplicas: 10
    template:
      spec:
        containers:
        - name: ray-worker
          image: rayproject/ray:2.5.0 # Use a standard Ray image
          resources:
          # I set this to a quite small number since I'm running it locally with 23 cores and 32GB RAM
            requests:
              cpu: "1"
              memory: "2Gi"
            limits:
              cpu: "23"
              memory: "32Gi"